{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11851874,"sourceType":"datasetVersion","datasetId":7447218},{"sourceId":11851879,"sourceType":"datasetVersion","datasetId":7447220}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy scipy gensim googletrans fitz pypdf pandas pyldavis nltk spacy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:56:34.572148Z","iopub.execute_input":"2025-05-17T20:56:34.572399Z","iopub.status.idle":"2025-05-17T20:56:56.369378Z","shell.execute_reply.started":"2025-05-17T20:56:34.572379Z","shell.execute_reply":"2025-05-17T20:56:56.367813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade googletrans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:57:25.026692Z","iopub.execute_input":"2025-05-17T20:57:25.027041Z","iopub.status.idle":"2025-05-17T20:57:28.957687Z","shell.execute_reply.started":"2025-05-17T20:57:25.027005Z","shell.execute_reply":"2025-05-17T20:57:28.956517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gensim\nfrom gensim import corpora\nfrom gensim.models import LdaModel, CoherenceModel\nimport random, numpy as np\nimport pandas as pd\nimport ast\n\n# 1. Seed all RNGs\nrandom.seed(42)\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:23:49.705245Z","iopub.execute_input":"2025-05-17T21:23:49.705564Z","iopub.status.idle":"2025-05-17T21:23:49.711161Z","shell.execute_reply.started":"2025-05-17T21:23:49.705540Z","shell.execute_reply":"2025-05-17T21:23:49.710104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_translated_documents = pd.read_csv(\"/kaggle/input/final-docs/final_docs_english (1).csv\")\ndf_params = pd.read_csv(\"/kaggle/input/train-results/results.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:23:51.962436Z","iopub.execute_input":"2025-05-17T21:23:51.963316Z","iopub.status.idle":"2025-05-17T21:23:52.262123Z","shell.execute_reply.started":"2025-05-17T21:23:51.963285Z","shell.execute_reply":"2025-05-17T21:23:52.261150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params_index = 69\nbest_params = df_params.iloc[best_params_index]\nbest_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:23:53.719467Z","iopub.execute_input":"2025-05-17T21:23:53.719795Z","iopub.status.idle":"2025-05-17T21:23:53.727702Z","shell.execute_reply.started":"2025-05-17T21:23:53.719772Z","shell.execute_reply":"2025-05-17T21:23:53.726753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params['num_topics'].dtype\nbest_params['passes'].dtype\nbest_params['iterations'].dtype\nbest_params['alpha'].dtype\nbest_params['eta'].dtype\nbest_params['coherence'].dtype\nbest_params['perplexity'].dtype","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:26:23.503821Z","iopub.execute_input":"2025-05-17T21:26:23.504337Z","iopub.status.idle":"2025-05-17T21:26:23.527382Z","shell.execute_reply.started":"2025-05-17T21:26:23.504294Z","shell.execute_reply":"2025-05-17T21:26:23.526366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_translated_documents['tokens'] = df_translated_documents['tokens'].apply(ast.literal_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:31:47.320481Z","iopub.execute_input":"2025-05-17T21:31:47.320825Z","iopub.status.idle":"2025-05-17T21:31:50.874259Z","shell.execute_reply.started":"2025-05-17T21:31:47.320801Z","shell.execute_reply":"2025-05-17T21:31:50.873295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# 4. Build Gensim dictionary and corpus\ndictionary = gensim.corpora.Dictionary(df_translated_documents['tokens'])\ndictionary.filter_extremes(no_below=2, no_above=0.5)\ncorpus = [dictionary.doc2bow(tokens) for tokens in df_translated_documents['tokens']]\n\n# 5. Split corpus for held-out perplexity\ntrain_corpus, heldout_corpus = train_test_split(corpus, test_size=0.05, random_state=42)\n\nmodel = LdaModel(\n    corpus=train_corpus,\n    id2word=dictionary,\n    num_topics   = best_params['num_topics'],\n    passes       = best_params['passes'],\n    iterations   = best_params['iterations'],\n    alpha        = 'auto',\n    eta          = 'auto',\n    random_state = 42\n)\n\ncoherence = CoherenceModel(\n                model=model,\n                texts=df_translated_documents['tokens'],\n                dictionary=dictionary,\n                coherence='c_v'\n            ).get_coherence()\n\nperplexity = model.log_perplexity(heldout_corpus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:32:02.435964Z","iopub.execute_input":"2025-05-17T21:32:02.436269Z","iopub.status.idle":"2025-05-17T21:32:39.722841Z","shell.execute_reply.started":"2025-05-17T21:32:02.436248Z","shell.execute_reply":"2025-05-17T21:32:39.721484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(model.num_topics):\n    terms = [term for term, weight in model.show_topic(i, topn=20)]\n    print(f\"Topic {i+1}: {', '.join(terms)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:32:57.130343Z","iopub.execute_input":"2025-05-17T21:32:57.130701Z","iopub.status.idle":"2025-05-17T21:32:57.151768Z","shell.execute_reply.started":"2025-05-17T21:32:57.130676Z","shell.execute_reply":"2025-05-17T21:32:57.150873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#saving model, dict and corpora as an extra\nmodel.save(\"best_ai_enviro_lda.model\")\ndictionary.save(\"best_ai_enviro.dict\")\n# optionally save the corpus in Matrix Market format:\nfrom gensim import corpora\ncorpora.MmCorpus.serialize(\"best_ai_enviro.mm\", train_corpus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:34:20.136917Z","iopub.execute_input":"2025-05-17T21:34:20.137281Z","iopub.status.idle":"2025-05-17T21:34:20.243376Z","shell.execute_reply.started":"2025-05-17T21:34:20.137257Z","shell.execute_reply":"2025-05-17T21:34:20.242333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk import sent_tokenize\n\nmappings = []  # will hold (filename, chunk_id, chunk_text, topic_id, topic_prob)\n\nfor _, row in df_translated_documents.iterrows():\n    text = row['text']\n    # 1) Break into chunks:\n    #    you can split on blank lines for paragraphs, or use sent_tokenize for sentences\n    paragraphs = [p.strip() for p in text.split('\\n\\n') if len(p.strip())>0]\n    for pid, para in enumerate(paragraphs):\n        tokens = preprocess(para)\n        bow    = dictionary.doc2bow(tokens)\n        # 2) Get full topic distribution for this chunk\n        dist   = best_model.get_document_topics(bow, minimum_probability=0)\n        # 3) Pick the highest‚Äêprobability topic\n        top_topic, top_prob = max(dist, key=lambda x: x[1])\n        mappings.append({\n            'filename':     row['filename'],\n            'country':      row['country'],\n            'chunk_id':     pid,\n            'text_snippet': para[:200],   # first 200 chars\n            'topic_id':     top_topic,\n            'topic_prob':   top_prob\n        })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:40:00.608529Z","iopub.execute_input":"2025-05-17T21:40:00.608906Z","iopub.status.idle":"2025-05-17T21:40:01.521670Z","shell.execute_reply.started":"2025-05-17T21:40:00.608881Z","shell.execute_reply":"2025-05-17T21:40:01.520279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4) Turn into a DataFrame\nchunk_df = pd.DataFrame(mappings)\n\n# 5) Filter only environment-related topics\nenv_topics = [0,2,12,15,20,21]  # whatever your env-topic IDs are\nenv_chunks = chunk_df[chunk_df['topic_id'].isin(env_topics)]\n\n# 6) Save to CSV\nchunk_df.to_csv('chunk_topic_map.csv', index=False)\nenv_chunks.to_csv('chunk_topic_map_environmental.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}