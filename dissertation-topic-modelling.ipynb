{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11704350,"sourceType":"datasetVersion","datasetId":7346613},{"sourceId":11718742,"sourceType":"datasetVersion","datasetId":7356292},{"sourceId":11721968,"sourceType":"datasetVersion","datasetId":7358470}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy scipy gensim googletrans fitz pypdf pandas pyldavis nltk spacy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:08:12.611711Z","iopub.execute_input":"2025-05-08T14:08:12.612095Z","iopub.status.idle":"2025-05-08T14:08:34.666966Z","shell.execute_reply.started":"2025-05-08T14:08:12.612061Z","shell.execute_reply":"2025-05-08T14:08:34.665863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade googletrans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:08:34.668152Z","iopub.execute_input":"2025-05-08T14:08:34.668627Z","iopub.status.idle":"2025-05-08T14:08:38.581027Z","shell.execute_reply.started":"2025-05-08T14:08:34.668589Z","shell.execute_reply":"2025-05-08T14:08:38.580081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pypdf import PdfReader\nfrom gensim import corpora\nfrom gensim.models import LdaModel, CoherenceModel\nimport random\nimport numpy as np\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom gensim.utils import get_random_state\n\n#setting random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\n\n# Attempt to import Googletrans; if unavailable, skip translation\ntry:\n    from googletrans import Translator\n    translator = Translator()\n    do_translate = True\n    print(\"googletrans imported; will translate non-English texts.\")\nexcept ImportError:\n    print(\"googletrans not installed; skipping translation step.\")\n    translator = None\n    do_translate = False\n\n# 1. Extract text from all PDFs under 'downloads'\nroot_dir = '/kaggle/input/strategies-new/downloads'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:08:38.583660Z","iopub.execute_input":"2025-05-08T14:08:38.583953Z","iopub.status.idle":"2025-05-08T14:09:02.500615Z","shell.execute_reply.started":"2025-05-08T14:08:38.583927Z","shell.execute_reply":"2025-05-08T14:09:02.499719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"documents = []\n\nfor dirpath, _, filenames in os.walk(root_dir):\n    for fname in sorted(filenames): #sorting to ensure same order each time\n        if fname.lower().endswith('.pdf'):\n            full_path = os.path.join(dirpath, fname)\n            # Extract country from directory structure\n            rel_path = os.path.relpath(full_path, root_dir)\n            parts = rel_path.split(os.sep)\n            country = parts[0] if len(parts) > 1 else 'Unknown'\n            #file_paths.append(full_path)\n            print(country, fname)\n            try:\n                doc = PdfReader(full_path)\n                print(len(doc.pages))\n                text = \"\"\n                for page in doc.pages:\n                    try:\n                        text += page.extract_text()\n                    except:\n                        print(fname + \"failed to fully extract\")\n                documents.append({\n                            'country': country,\n                            'filename': fname,\n                            'text': text\n                        })\n                doc.close()\n            except:\n                print(\"Could not read\" + fname)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:09:02.501712Z","iopub.execute_input":"2025-05-08T14:09:02.502327Z","iopub.status.idle":"2025-05-08T14:20:54.274097Z","shell.execute_reply.started":"2025-05-08T14:09:02.502294Z","shell.execute_reply":"2025-05-08T14:20:54.272116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_documents = pd.DataFrame(documents)\ndf_documents.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:20:54.275981Z","iopub.execute_input":"2025-05-08T14:20:54.276486Z","iopub.status.idle":"2025-05-08T14:20:54.325189Z","shell.execute_reply.started":"2025-05-08T14:20:54.276452Z","shell.execute_reply":"2025-05-08T14:20:54.324084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_documents = df_documents.applymap(lambda x: x.encode('unicode_escape').\n                 decode('utf-8') if isinstance(x, str) else x)\ndf_documents.to_excel(\"Untranslated_docs1.xlsx\", index=False,header=True)\ntry:\n    df_documents.to_csv(\"Untranslated_docs1.csv\", index=False,header=True)\nexcept:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:20:54.326178Z","iopub.execute_input":"2025-05-08T14:20:54.326627Z","iopub.status.idle":"2025-05-08T14:20:55.472144Z","shell.execute_reply.started":"2025-05-08T14:20:54.326595Z","shell.execute_reply":"2025-05-08T14:20:55.470572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df_documents = pd.read_excel(\"/kaggle/input/untranslated-docs/Untranslated_docs1.xlsx\")\n#df_documents.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:20:55.474189Z","iopub.execute_input":"2025-05-08T14:20:55.477272Z","iopub.status.idle":"2025-05-08T14:20:55.487748Z","shell.execute_reply.started":"2025-05-08T14:20:55.477230Z","shell.execute_reply":"2025-05-08T14:20:55.483214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import textwrap\nimport asyncio\n\nasync def translate_long_text(text, dest=\"en\", chunk_size=4000):\n    \"\"\"\n    Splits `text` into chunks of at most `chunk_size` characters,\n    translates each chunk, and returns the rejoined translated text.\n    \"\"\"\n    # textwrap.wrap will split on whitespace boundaries\n    chunks = textwrap.wrap(text, chunk_size)\n    translated_chunks = []\n    for chunk in chunks:\n        # translate() both detects and translates\n        async with Translator() as translator:\n            translated = await translator.translate(chunk, dest=dest)\n            if translated.src == \"en\":\n                return text # return original text if english\n            translated_chunks.append(translated.text)\n    return \" \".join(translated_chunks)\n\n# 2. Translate to English if possible\ntranslated_documents = []\nfor doc in documents:\n    if do_translate:\n        print(doc['filename'])\n        translation = await translate_long_text(doc['text'])\n        doc['text'] = translation\n        print('done')\n        #except:\n            #print(\"Translation failed for \" + doc['country'] + \" , \" + doc['filename'])\n        # otherwise, leave entry['text'] as-is\n        translated_documents.append(doc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:20:55.489817Z","iopub.execute_input":"2025-05-08T14:20:55.490307Z","iopub.status.idle":"2025-05-08T14:31:55.860859Z","shell.execute_reply.started":"2025-05-08T14:20:55.490259Z","shell.execute_reply":"2025-05-08T14:31:55.859969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_translated_documents = pd.DataFrame(translated_documents)\ndf_translated_documents = df_translated_documents.applymap(lambda x: x.encode('unicode_escape').\n                 decode('utf-8') if isinstance(x, str) else x)\ndf_translated_documents.to_excel(\"translated_docs3.xlsx\", index=False,header=True)\ntry:\n    df_translated_documents.to_csv(\"translated_docs3.csv\", index=False,header=True)\nexcept:\n    pass\ndf_translated_documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:31:55.861751Z","iopub.execute_input":"2025-05-08T14:31:55.861976Z","iopub.status.idle":"2025-05-08T14:31:56.280571Z","shell.execute_reply.started":"2025-05-08T14:31:55.861957Z","shell.execute_reply":"2025-05-08T14:31:56.279757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"italian = df_translated_documents[df_translated_documents[\"country\"] == 'Italy']\nitalian","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:31:56.281458Z","iopub.execute_input":"2025-05-08T14:31:56.281729Z","iopub.status.idle":"2025-05-08T14:31:56.292491Z","shell.execute_reply.started":"2025-05-08T14:31:56.281704Z","shell.execute_reply":"2025-05-08T14:31:56.291612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install unidecode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:31:56.293287Z","iopub.execute_input":"2025-05-08T14:31:56.293638Z","iopub.status.idle":"2025-05-08T14:32:00.545769Z","shell.execute_reply.started":"2025-05-08T14:31:56.293617Z","shell.execute_reply":"2025-05-08T14:32:00.544568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#preparing for LDA\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport spacy\nfrom unidecode import unidecode\nfrom nltk.corpus import words\n\nenglish_vocab = set(words.words())\n\ndef remove_unicode(text: str) -> str:\n    \"\"\"\n    Transliterate *all* Unicode to ASCII (e.g. Å„ â†’ n, ð¢ â†’ i, Ã¼ â†’ u),\n    then remove any leftover nonâ€printable characters and collapse whitespace.\n    \"\"\"\n    # 1. Unidecode: full transliteration\n    text_ascii = unidecode(text)\n    # 2. Remove any remaining non-ASCII (should be rare now)\n    text_ascii = text_ascii.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    # 3. Strip control/non-printable characters\n    text_ascii = re.sub(r\"[^\\x20-\\x7E]\", \" \", text_ascii)\n    # 4. Collapse whitespace\n    text_ascii = re.sub(r\"\\s+\", \" \", text_ascii).strip()\n    return text_ascii\n\ndef remove_country_name(text: str, country: str) -> str:\n    \"\"\"\n    Remove occurrences of the country name (word boundary) from the text, case-insensitive.\n    \"\"\"\n    pattern = r'\\b{}\\b'.format(re.escape(country))\n    cleaned = re.sub(pattern, '', text, flags=re.IGNORECASE)\n    # Collapse whitespace\n    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n    return cleaned\n\ndef fully_clean(text: str) -> str:\n    # 1. Decode any literal Python/JSON-style escapes (\\n, \\u2019, etc.)\n    try:\n        text = text.encode('utf-8').decode('unicode_escape')\n    except Exception:\n        # if it fails (e.g. because it's already real newlines), just carry on\n        pass\n\n    # 2. Now replace real newlines/tabs with spaces\n    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n\n    # 3. Transliterate everything to ASCII (Å„â†’n, ð¢â†’i, â€™â†’')\n    text = unidecode(text)\n\n    # 4. Drop any remaining non-printable or non-ASCII (just in case)\n    text = re.sub(r'[^\\x20-\\x7E]', ' ', text)\n\n    # 5. Collapse multiple spaces into one\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n\ndef clean_text(text):\n    #text = text.replace('\\r\\n', '')\n    text = text.replace('\\n', '')  # Newline\n    text = text.replace('\\r', '')  # Carriage return\n    text = text.replace('\\t', '')  # Tab\n    text = text.replace('\\b', '')  # Backspace\n    text = text.replace('\\f', '')  # Form feed\n    text = text.replace('\\a', '')  # Alert sound\n    text = text.replace('\\\\', '')  # Literal backslash\n    #text = text.replace('\\\\n', '')  # Newline\n    #text = text.replace('\\\\r', '')  # Carriage return\n    #text = text.replace('\\\\t', '')  # Tab\n    #text = text.replace('\\\\b', '')  # Backspace\n    #text = text.replace('\\\\f', '')  # Form feed\n    #text = text.replace('\\\\a', '')  # Alert sound\n    #text = text.replace('\\\\\\\\', '')  # Literal backslash\n    #text = re.sub(r'\\S+@\\S+', '', text)\n    #text = re.sub(r'http\\S+', '', text)\n    #text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    return text.strip().lower()\n\nstop_words = set(stopwords.words('english'))\ndef preprocess(text):\n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n    return tokens\n\nnlp = spacy.load('en_core_web_sm')\ndef lemmatize(tokens):\n    doc = nlp(' '.join(tokens))\n    return [token.lemma_ for token in doc if token.pos_ in ['NOUN', 'ADJ', 'VERB']]\n\nfrom gensim.utils import simple_preprocess\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\n\n# 1. Ensure stopwords are available\nnltk.download('stopwords')\n\n# 2. Load spaCy model\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n\n# 3. Prepare stopword set\nstop_words = set(stopwords.words('english'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:32:00.547547Z","iopub.execute_input":"2025-05-08T14:32:00.547952Z","iopub.status.idle":"2025-05-08T14:32:12.465343Z","shell.execute_reply.started":"2025-05-08T14:32:00.547910Z","shell.execute_reply":"2025-05-08T14:32:12.464411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df_translated_documents['text'] = clean_text(df_translated_documents['text'])\n#df_translated_documents['text'] = preprocess(df_translated_documents['text'])\n#df_translated_documents['text'] = lemmatize(df_translated_documents['text'])\ndef preprocess_text(text):\n    # Tokenize, lowercase, remove punctuation\n    text = clean_text(text)\n    tokens = simple_preprocess(text, deacc=True)\n    # Remove stopwords\n    tokens_nostop = [tok for tok in tokens if tok not in stop_words and len(tok) > 3]\n    # Lemmatize with spaCy\n    doc = nlp(\" \".join(tokens_nostop))\n    \n    lemmas = [\n        token.lemma_ for token in doc\n        if token.lemma_.isalpha() and token.lemma_ not in stop_words\n    ]\n    return lemmas\n\ndf_translated_documents['tokens'] = df_translated_documents['text'].apply(fully_clean)\ndf_translated_documents['tokens'] = df_translated_documents.apply(lambda row: remove_country_name(row['tokens'], row['country']), axis=1)\ndf_translated_documents['tokens'] = df_translated_documents['tokens'].apply(preprocess_text)\ndf_translated_documents['tokens'] = df_translated_documents['tokens'].apply(lambda toks: [t for t in toks if t in english_vocab]) # remove remaining non-english / random characters\ndf_translated_documents.head()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:32:12.466334Z","iopub.execute_input":"2025-05-08T14:32:12.467099Z","iopub.status.idle":"2025-05-08T14:33:56.564166Z","shell.execute_reply.started":"2025-05-08T14:32:12.467071Z","shell.execute_reply":"2025-05-08T14:33:56.562070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_translated_documents.to_excel('final_docs_english.xlsx',index=False)\ntry:\n    df_translated_documents.to_csv('final_docs_english.csv',index=False)\nexcept:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:33:56.566062Z","iopub.execute_input":"2025-05-08T14:33:56.566513Z","iopub.status.idle":"2025-05-08T14:33:57.327643Z","shell.execute_reply.started":"2025-05-08T14:33:56.566477Z","shell.execute_reply":"2025-05-08T14:33:57.326505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gensim\n# 4. Build Gensim dictionary and corpus\ndictionary = gensim.corpora.Dictionary(df_translated_documents['tokens'])\ndictionary.filter_extremes(no_below=2, no_above=0.5)\ncorpus = [dictionary.doc2bow(tokens) for tokens in df_translated_documents['tokens']]\n\n# 5. Split corpus for held-out perplexity\ntrain_corpus, heldout_corpus = train_test_split(corpus, test_size=0.05, random_state=42)\n\n# 6. Grid search over LdaModel hyperparameters with coherence & perplexity\nresults = []\nbest_coherence = -np.inf\nbest_model = None\nbest_params = {}\nalpha='auto'\n\nfor num_topics in [5, 10, 15, 20, 25]:\n    for passes in [10, 20, 30, 35, 40]:\n        for iterations in [25,50,100]:\n            print(\"Training model for \",num_topics,\" topics\")\n            model = LdaModel(\n                corpus=train_corpus,\n                id2word=dictionary,\n                num_topics=num_topics,\n                alpha='auto',\n                eta='auto',\n                passes=passes,\n                iterations=iterations,\n                random_state=42\n            )\n            coherence = CoherenceModel(\n                model=model,\n                texts=df_translated_documents['tokens'],\n                dictionary=dictionary,\n                coherence='c_v'\n            ).get_coherence()\n            perplexity = model.log_perplexity(heldout_corpus)\n            results.append({\n                'num_topics': num_topics,\n                'passes' : passes,\n                'iterations' : iterations,\n                'alpha': model.alpha,\n                'eta' : model.eta,\n                'coherence': coherence,\n                'perplexity': perplexity\n            })\n            print(results[-1])\n            for i in range(model.num_topics):\n                terms = [term for term, weight in model.show_topic(i, topn=20)]\n                print(f\"Topic {i+1}: {', '.join(terms)}\")\n            if coherence > best_coherence:\n                best_coherence = coherence\n                best_model = model\n                best_params = {\n                    'num_topics': num_topics,\n                    'passes' : passes,\n                    'iterations' : iterations,\n                    'alpha': model.alpha,\n                    'eta': model.eta,\n                    'coherence': coherence,\n                    'perplexity': perplexity\n                }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:33:57.329206Z","iopub.execute_input":"2025-05-08T14:33:57.329609Z","iopub.status.idle":"2025-05-08T15:35:53.124402Z","shell.execute_reply.started":"2025-05-08T14:33:57.329579Z","shell.execute_reply":"2025-05-08T15:35:53.121623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Display grid search results\n#import ace_tools as tools\ndf_results = pd.DataFrame(results)\n#tools.display_dataframe_to_user(name=\"LDA Grid Search Results\", dataframe=df_results)\n\n# 8. Show best model parameters and top topic terms\nprint(\"Best Model Parameters:\", best_params)\nfor i in range(best_model.num_topics):\n    terms = [term for term, weight in best_model.show_topic(i, topn=20)]\n    print(f\"Topic {i+1}: {', '.join(terms)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T15:35:53.127022Z","iopub.execute_input":"2025-05-08T15:35:53.127843Z","iopub.status.idle":"2025-05-08T15:35:53.148545Z","shell.execute_reply.started":"2025-05-08T15:35:53.127779Z","shell.execute_reply":"2025-05-08T15:35:53.147520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_results.to_excel('results.xlsx',index=False)\ntry:\n    df_results.to_csv('results.csv',index=False)\nexcept:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T15:35:53.149799Z","iopub.execute_input":"2025-05-08T15:35:53.150120Z","iopub.status.idle":"2025-05-08T15:35:53.214563Z","shell.execute_reply.started":"2025-05-08T15:35:53.150097Z","shell.execute_reply":"2025-05-08T15:35:53.212429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis\n\n# 1. Compute full document-topic distributions\ndoc_topic_probs = []\nfor bow in corpus:\n    # Get probability for every topic in the model\n    topic_dist = dict(best_model.get_document_topics(bow, minimum_probability=0))\n    doc_topic_probs.append(topic_dist)\n\n# 2. Convert to DataFrame\ntopic_df = pd.DataFrame(doc_topic_probs).fillna(0)\n# Rename columns to topic_0, topic_1, ...\ntopic_df.columns = [f\"topic_{i}\" for i in topic_df.columns]\n\n# 3. Combine with metadata\noutput_df = pd.concat(\n    [df_documents[['country', 'filename']].reset_index(drop=True), topic_df],\n    axis=1\n)\n\noutput_df.to_excel(\"topic_dist_per_doc.xlsx\", index=False)\noutput_df.to_csv(\"topic_dist_per_doc.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vis_data = gensimvis.prepare(best_model, corpus, dictionary)\npyLDAvis.enable_notebook()\n\nvis_data","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}